\documentclass{scrartcl}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{tikz-cd}
%\usepackage{mathtools}

\usepackage[UKenglish]{babel}
\usepackage[UKenglish]{isodate}
\cleanlookdateon

\usepackage{hyperref} %last one to import

\def\le{\leqslant}
\def\ge{\geqslant}

%\def\C{\mathcal{C}}
\def\Z{\mathbb{Z}}
%\def\N{\mathbb{N}}
\def\R{\mathbb{R}}
\def\C{\mathbb{C}}
\def\Q{\mathbb{Q}}
\def\L{\mathcal{L}}
\def\B{\mathcal{B}}

\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\charf}{char}
\DeclareMathOperator{\id}{Id}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\rank}{rk}
\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\sgn}{sgn}
 %\char is taken, by something unrelated
\DeclareMathOperator{\spn}{span}  %\span is taken, by something unrelated


\begin{document}

\newtheorem{theorem}{Theorem}[subsection]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}

%\theoremstyle{definition}
\newtheorem{definition}{Definition}[subsection]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{example}{Example}
\newtheorem*{examples}{Examples}

\title{Linear Algebra II (S)}
\author{Deng Tianle}
\date{\today}
\maketitle

\tableofcontents

\section{Introduction}

This is basically my notes based on the lectures by Dr. Arghya Sadhukhan for MA2101S at the National University of Singapore. All mistakes are almost certainly mine. 
\par 
The main reference book is the standard textbook by Hoffman and Kunze. 

\section{Preliminaries}
\subsection{Cardinality}
We give a rough collection of results For details, please refer to the materials in MA1100T. 
\begin{definition}
A set $S$ is countable if $S$ is finite, or there exists a bijection $\phi: \Z_{\ge 0} \to S$. 
\end{definition}
\begin{remark}
Roughly, $S \text{ is countable} \iff \exists \text{(infinite) algorithm to list its elements}$. 
\end{remark}
\begin{proposition} \label{countability}
Take $S \ne \emptyset$. The following are equivalent: 
\begin{enumerate}
	\item $S$ is conutable
	\item there exists an injection $\phi: S \to \Z_{\ge 0}$
	\item there exists a countable set $C$ and an injection $\psi:S \to C$.
\end{enumerate}
\end{proposition}

\begin{corollary}
	Every subset of a countable set is countable too. 
\end{corollary}
\begin{proposition}
	A finite product of countable sets is also countable.
\end{proposition}
\begin{proof}
	Let $S_1, \dots, S_n$ be countable sets. Let $p_1, \dots, p_n$ be distinct primes. We have injections $f_i: S_i \to \Z_{\ge 0}$ by 2. of Proposition \ref{countability}. Define 
	\begin{align*}
		\phi:  S_1 \times \dots \times S_n &\to \Z_{\ge 0} \\
		 (s_1, \dots, s_n) &\mapsto p_1^{f_1(s_1)}\dots p_n^{f_n(s_n)}.
	\end{align*}
	We claim that $\phi$ is injective. Indeed, we have
	\begin{align*}
		&p_1^{f_1(s_1)}\dots p_n^{f_n(s_n)} = p_1^{f_1(s'_1)}\dots p_n^{f_n(s'_n)} \\
		\overset{\text{FTA}}{\implies} & f_i(s_i) = f_i(s'_i) \,\forall i=1, \dots, n \\
		\overset{\text{injectivity}}{\implies} & s_i = s'_i \,\forall i=1, \dots, n \\
		\implies & (s_1, \dots, s_n) = (s'_1, \dots, s'_n). 
	\end{align*}
\end{proof}
\begin{corollary}
	$\Q$ is countable.
\end{corollary}
\begin{proposition}
	The union of a countable collection of countable sets is countable.
\end{proposition}
\begin{lemma}
	Let $S \ne \emptyset$, $f: S \to \mathcal{P}(S)$; then $f$ cannot be surjective.
\end{lemma}
\begin{proof}
	Suppose $f$ is surjective. Consider $X:= \{s \in S : s \notin f(s)\}$. Then $X \in \mathcal{P}(S)$, so by assumption there exists $x \in S$ such that $f(x) = X$. Then we get $x \in X \iff x \notin X$, contradiction. 
\end{proof}
\begin{corollary}
	$\mathcal{P}(\Z_{\ge 0})$ is countable.
\end{corollary}
\begin{corollary}
	$\R$ is uncountable.
\end{corollary}

\subsection{Groups, Rings and Fields}
\begin{definition}
	A binary operation on $S \ne \emptyset$ is a function $*: S\times S \to S$, $(a, b) \mapsto a * b$. 
\end{definition}
\begin{remark}
	Addition, subtraction and multiplcation on $\R$ are examples. Division on $\R$ is a non-example. 
\end{remark}
\begin{definition}
	We have the following terminologies:
	\begin{enumerate}
		\item $*$ is associative: $(a*b)*c = a*(b*c)$ for all $a, b, c \in S$
		\item $*$ is commutative: $a*b = b*a$ for all $a, b \in S$
		\item an identity $e \in S$ is an element such that $a*e = e*a = a$ for all $a \in S$
		\item if $e\in S$ is the identity, then an inverse of $a \in S$ is an element $a^{-1} \in S$ such that $a*b = b*a = e$. 
	\end{enumerate}
\end{definition}
\begin{remark}
	The identity is unique if it exists. The inverse is also unique if it exists (this justifies the notation $a^{-1}$). We have $(a^{-1})^{-1} = a$.
\end{remark}
\begin{definition}
	Let $G \ne \emptyset$ be a set and $*$ be a binary operation on $G$. If $*$ is associative with the existence of identity and existence of inverses for each element in $G$, we say that $(G, *)$ is a group. If $*$ is commutative, we say that $G$ is abelian.
\end{definition}
\begin{example}
	\begin{enumerate}
		\item An example is $(\R, +)$. A non-example is $(R, -)$.
		\item Let $p$ be a prime, $\Z_p := \{0, 1, \dots, p-1\}$.
			\begin{enumerate}
				\item $(\Z_p, +)$ is a group, where $+ = \text{addition modulo p}$; more generally $(\Z_n, +)$ is also a group for any $n \in \Z_{\ge 0}$
				\item $(\Z_p - \{0\}, \cdot)$ is a group
			\end{enumerate} 
		\item $(M_n(\R). +)$ is an abelian group
		\item $(\text{set of invertible matrices in }M_n(\R), \cdot)$ is a non-abelian group denoted as $\text{GL}_n(\R)$. 
	\end{enumerate}
\end{example}
\begin{definition}
	Let $R$ be a ring. Let $+$ and $\cdot$ be two associative binary operations on $R$ such that: 
	\begin{enumerate}
		\item $(R, +)$ is an abelian group (we denote the additive identity as $0$; for $a\in R$, we denote its additive inverse as $-a$)
		\item for all $a, b, c \in R$, $a\cdot(b+c) = a\cdot b + a \cdot c$, $(b+c)\cdot a = b \cdot a + c\cdot a$
	\end{enumerate}
\end{definition}
\begin{example}
	Both $(\Z_p, +, \cdot)$ and $(M_n(\R), +, \cdot)$ are rings. Let $R$ be a ring and $R[x]$ be the set of polynomials with coefficients in $R$. Then $(R[x], +, \cdot)$ is a ring. 
\end{example}
\begin{lemma}
	Let $R$ be a ring with identity $1$. Let $a \in R$. 
	\begin{enumerate}
		\item $0 \cdot a = a \cdot 0 = 0$
		\item $(-1)\cdot a = -a = a \cdot (-1)$
	\end{enumerate}
\end{lemma}
\begin{definition}
	Let $F$ be a ring with the additional property that $(F-\{0\}, \cdot)$ is an abelian group (with identity $1$). Then we say that $(F, +, \cdot)$ is a field. 
\end{definition}
\begin{remark}
	Let $F$ be a field. For all $a, b \in F$, we have $a, b \ne 0 \implies a\cdot b \ne 0$ (because $F-\{0\}$ is closed with respect to $\cdot$). There are at least two elements $0\ne 1$ in $F$. 
\end{remark}
\begin{example}
	Common examples of fields include $\R$, $\Q$, $\C$, $F_p = (\Z_p, +, \cdot)$. 
\end{example}
Let $F$ be a field. In the following discussion, to avoid confusion, we write $0_F$ and $1_F$ for the additive and multiplicative identities respectively. We would also abbreviate $\underbrace{1_F+1_F+\dots + 1_F}_{n \text{ copies}}$ as $n \cdot 1_F$ and omit the $\cdot$ for field multiplication. 
\begin{definition}
	We define the characteristic of a field $F$, 
	\[\charf{F} := \begin{cases}
		\min \{n\in \Z_{>0}: n\cdot 1_F = 0_F\}, & \text{if this set is non-empty} \\
		0, & \text{otherwise}.
	\end{cases}\]
\end{definition}
\begin{example}
	We have $\charf{\R}=0$, $\charf{F_p}=p$.
\end{example}
\begin{lemma}
	Let $F$ be a field. Then $\charf{F}$ is either $0$ or a prime $p$.
\end{lemma}
\begin{proof}
	Assume $\charf{F} \ne 0$. There exists a minimal $n \in \Z_{>0}$ such that $n \cdot 1_F = 0_F$. Write $n=ab$, we have $(ab) \cdot 1_F = (a\cdot 1_F)(b\cdot 1_F) = 0_F$, so $a\cdot 1_F = 0_F$ or $b\cdot 1_F = 0_F$. By minimality, $n$ has to be prime. 
\end{proof}
\begin{remark}
	Let $F$ be a finite field. It is not hard to show that if $\charf F = p$, then $|F| = p^n$ for some $n$. 
\end{remark}

\subsection{Polynomials}
Let $R$ be a ring. We define
\[ R[x] := \left\{\sum_{i \in I}a_ix^i : I \subset \Z_{\ge 0}, |I|<\infty, a_i \in R \, \forall i \in I\right\}.\]
It is clear that $(R[x], +, \cdot)$ is a ring, called the polynomial ring of $R$. We have a natural embedding of $R$ into $R[x]$ by the rule $a \mapsto ax^0$.
Let $f \in R[x]$. We define the degree of $f$, $\deg{f} = \max\{i\in I : a_i \ne 0\}$ for $f \ne 0$ and $\deg{0} = -\infty$. The elements of $R$ are called constant polynomials. 
\begin{remark}
	We have $\deg(f\cdot g) = \deg{f}+\deg{g}$.
\end{remark}
\begin{remark}
	The lecture did not require $R$ to be commutative. Note that when $R$ is noncomutative, the evaluation map may not be a ring homomorphism anymore. 
\end{remark}
Assume now that $R=F$ is a field. We want to study $F[x]$. In this special case, we have the following proposition:
\begin{proposition}[Division algorithm]
	Let $f(x), g(x) \in F[x]$, $g(x) \ne 0$. Then there exists unique polynomials $q(x), r(x) \in F[x]$ such that:
	\begin{enumerate}
		\item $f(x) = q(x)g(x)+r(x)$
		\item (either $r(x)=0$ or) $\deg{r}<\deg{g}$.
	\end{enumerate}
\end{proposition}
In fact, one can generalise the concept of a division algorithm. 
\begin{definition}
	An integral domain $R$ is Euclidean if there exists a function $d:R-\{0\} \to \Z_{>0}$ such that for all $a, b \in R$, $b \ne 0$, there exists $q, r \in R$ such that
	\[a = bq+r, \]
	with $r=0$ or $d(r) < d(b)$.
\end{definition}
\begin{example}
	The following are all Euclidean domains:
	\begin{enumerate}
		\item $R=F[x]$, $d=\deg$
		\item $R=\Z$, $d=|\,|$
		\item $R=\Z[i] := \{a+bi:a, b\in \Z\}$, $d: a+bi \mapsto a^2+b^2$.
	\end{enumerate}
\end{example}
\begin{remark}
	For Euclidean domains, we do not require $q$ and $r$ to be unique. However, observe that the uniqueness holds for $F[x]$. It turns out that this gives a characterisation of $F[x]$: an Euclidean domain with given $d$ is either a field or a polynomial domain over a field, if the $q$ and $r$ referred to above are unique. 
\end{remark} 
We now end the diversion and come back to the basic concepts for polynomials. 
\begin{definition}
	Let $(R, +, \cdot)$ be a ring. The invertible elements in $(R-\{0\}, \cdot)$ are called units. 
\end{definition}
\begin{definition}
	Let $F$ be a field and let $f \in F[x]$ be a polynomial. We say that $f$ is irreducible if $\deg{f} \ge 1$ and $f$ cannot be written as $f= g\cdot h$ where both $g$ and $h$ are nonconstant polynomials. We say that $f$ is monic if its leading coefficient (that is, the coefficient for $x^{\deg f}$) is $1_F$. 
\end{definition}
It is helpful to consider the follwing analogies:
\begin{center}
	\begin{tabular}{ c|c } 
		$\Z$ & $F[x]$ \\
		\hline
		$\pm 1$ & non-zero constants (ring units) \\ 
		prime & irreducible \\ 
		$|n|$ & $\deg{f}$ (for division algorithm) \\ 
		positive & monic polynomials
	\end{tabular}
\end{center}
Also, recall the usual chain of reasoning on $\Z$:
\begin{align*}
	\text{division algorithm} &\to \text{Bezout identity}  \\
	&\to \text{Euclid lemma} \\
	&\to \text{uniqueness of prime factorisation}.
\end{align*}
We now adapt these considerations to $F[x]$.
\begin{definition}
	Let $f, g \in F[x]$. We define $\gcd(f, g)$ to be the unique polynomial such that:
	\begin{enumerate}
		\item it is a common divisor of $f$ and $g$
		\item it has the maximum degree among all such common divisors
		\item it is monic.
	\end{enumerate}
	We say that $f, g$ are coprime if $\gcd(f, g)=1$.
\end{definition}
It is clear that the Bezout identity holds with this definition of $\gcd$. In fact, the usual chain of arguments holds with little modification. Similarly, we can define the $\gcd$ of multiple polynomials. We now record a lemma which (perhaps surprisingly) will be extremely useful later:
\begin{lemma}[Generalised Bezout] \label{bezout}
	Let $g_1, \dots, g_n \in F[x]$ be such that $\gcd(g_1, \dots, g_n)=1$. Then there exists $f_1, \dots, f_n \in F[x]$ such that $\sum_{i=1}^{n}f_i(x)g_i(x) = 1$.
\end{lemma}
\begin{proof}
	Consider the ideal $I$ generated by $g_1, \dots, g_n$. Because $F[x]$ is a PID, $I$ is generated by a single polynomial. This generator is a common divisor of all $g_i$, so in this case, we see that $1$ generates $I$, so in particular $1 \in I$. 
\end{proof}
\begin{example}
	To calculate $\gcd(x^2+1, x+1)$ in $\R[x]$, we have
	\[x^2+1 = (x-1)(x+1)+2\]
	\[x+1 = \left(\frac{1}{2}x+\frac{1}{2}\right)(2) + 0\]
	So $\gcd(x^2+1, x+1)=1$ (monic polynomial). 
\end{example}
\begin{theorem}
	Let $f \in F[x]$ be non-zero. Then we can write
	\[f = af_1\cdot \dots \cdot f_r\]
	where $a \in F - \{0\}$, $f_i \in F[x]$ monic irreducible and $r \in \Z_{\ge 0}$. Moreover, this is unique up to reordering. 
\end{theorem}

\begin{definition}
	Let $F$ be a field. We say that $F$ is algebraically closed if every non-constant polynomial in $F[x]$ has at least one root. (We say that $\xi$ is a root of $f \in F[x]$ if $f(\xi) = 0$.)
\end{definition}
\begin{example}
	\begin{enumerate}
		\item $\C$ is algebraically closed; in fact, $f \in \C[x]$ has exactly $\deg f$ many roots
		\item $\R$ is not algebraically closed, e.g. $f(x)=x^2+1$
		\item In $\Z/8\Z[x]$, consider $f(x)=x^2-1$, then $x=1, 3, 5, 7$ are roots even though $deg f = 2$.
	\end{enumerate}
\end{example}
\begin{lemma}
	$F$ is algebraically closed if and only if every irreducible polynomial has $\deg$ $1$. 
\end{lemma}
\begin{example}
	\begin{enumerate}
		\item In $\C[x]$, irreducibles are linear polynomials like $x-\xi$
		\item In $\R[x]$, $x^2+1$ is also irreducible; in fact, one can show that irreducibles are linear or quadratic in this case. 
	\end{enumerate}
\end{example}
\begin{proposition}
	Let $F$ be a field, let $m \in F^\times - (F^\times)^2$ (where $F^\times$ denotes $F -\{0\}$ and the whole notation denotes elements $m$ such that $x^2-m$ has no root in $F$). Then $K=F(\sqrt{m}):=\{a+b\sqrt{m}: a, b \in F\}$ is a field containing $F = \{a+0\sqrt{m}: a\in F\}$ as a subfield. Here addition and multiplcation are:
	\[(a+b\sqrt{m})+(c+d\sqrt{m}) = (a+c) + (b+d)\sqrt{m},\]
	\[(a+b\sqrt{m})(c+d\sqrt{m}) := (ac+bdm) + (ad+bc)\sqrt{m}.\]
\end{proposition}
\begin{proof}
	One can check the field axioms one by one. We only provide the check for multiplicative inverse: if $a+b\sqrt{m} \ne 0+0\sqrt{m} = 0$, then
	\[\frac{1}{a+b\sqrt{m}} = \frac{a-b\sqrt{m}}{(a+b\sqrt{m})(a-b\sqrt{m})} = \frac{a}{a^2-mb^2}-\frac{b}{a^2-mb^2}\sqrt{m}, \]
	where $a^2-mb^2 \ne 0$ due to assumptions about $a$, $b$ and $m$. 
\end{proof}
\begin{remark}
	In particular, for $F=\Z_p$, $K$ is a field with $p^2$ elements. 
\end{remark}
\begin{remark}
	$\Z_p(\sqrt{m}) \cong \Z_p / (x^2-m)$.
\end{remark}
\begin{theorem}
	Let $f$ be an irreducible polynomial of degree $n$ in $\Z_p[x]$. Then $\Z_p[x]/(f)$ is a field of order $p^n$. 
\end{theorem}

\section{Basic concepts}
\subsection{Vector spaces}
\begin{definition}
	Let $F$ be a field. A vector space $V$ over $F$ is a non-empty set with
	\begin{enumerate}
		\item vector addition: $V \times V \to V$, $(v_1, v_2) \mapsto v_1+v_2$
		\item scalar multiplication: $F \times V \to V$, $(\lambda, v) \mapsto \lambda v$
	\end{enumerate}
	such that: 
	\begin{enumerate}
		\item $(V, +)$ is an abelian group; call $0$($=0_V$) the identity element; $-v := \text{inverse of } v \in V$.
		\item associative law for scalar multiplication: for all $\lambda, \mu \in F$, $v \in V$
			\[\lambda(\mu v) = (\lambda\mu)v\]
		\item distributive laws: for all $\lambda, \mu \in F$, $u, v \in V$
			\[(\lambda+\mu)v = \lambda v + \mu v\]
			\[\lambda(u+v) = \lambda u + \lambda v\]
		\item $1_F v = v$ for all $v \in V$.
	\end{enumerate}
\end{definition}
\begin{lemma}
	\begin{enumerate}
		\item $0_Fv = 0_V$
		\item $\lambda 0_V = 0_V$
		\item $(-1_F)v = -v$
	\end{enumerate}
\end{lemma}
\iffalse
\begin{proof}
	\begin{enumerate}
		\item We have
		\begin{align*}
			0_F + 0_F = 0_F &\implies (0_F + 0_F)v = 0_F v \\
			&\implies 0_F v + 0_F v = 0_F v \\
			&\implies 0_F v + 0_F v - 0_F v = 0_F v - 0_F v\\
			&\implies 0_F v = 0_V.
		\end{align*}
		\item 
	\end{enumerate}
\end{proof}
\fi

\begin{example}
	\begin{enumerate}
		\item $V=F$; more generally, $V=F^n$
		\item $V = \Hom(F, F)$; more generally, for any set $X$, V = $\Hom(X, F)$, where:
		\[(f+g)(x) := f(x) + g(x)\]
		\[(\lambda f)(x) := \lambda f(x).\]
		\item $V=F[x]$
		\item $V=M_{m \times n}(F)$
		\item \begin{align*}
			V &= \text{set of sequences with entries in $F$} \\
			&= \{(a_1, \dots, a_i, \dots): a_i \in F \,\forall i\in \Z_{\ge 0}\}
		\end{align*}
		\item \[V' = \{(a_1, \dots, a_i, \dots): a_i \in F \,\forall i\in \Z_{\ge 0}, \exists N \in \Z_{\ge 0} \text{ such that } a_i=0 \,\forall i \ge N\}\]
		We have $V' \subset V$. Note that this is isomorphic to $V=F[x]$.
		\item If $F = \R, \C$ (fields with `analytic/topological flavour'). We can talk about functions with extra properties, e.g. $V=C^\infty([0, 1])$, $V=C^\infty(\R)$. 
	\end{enumerate}
\end{example}
\begin{definition}
	Let $V$ be an $F$-vector space, $W \subset F$. We say that $W$ is a subspace of $V$ if $W$ is a vector space itself with the vector addition and scalar multiplication on $V$. 
\end{definition}
\begin{proposition}
	$W$ is a subspace of $V$ if and only if: 
	\begin{enumerate}
		\item $W \ne \emptyset$
		\item $W$ is closed with respect to vector addition and scalar multiplication i.e. whenever $w_1, w_2 \in W$, $\lambda \in F$, we must have $w_1+w_2 \in W$ and $\lambda w_i \in W$.  
	\end{enumerate}
\end{proposition}
\begin{remark}
	For the closure above, it suffices to check that $w_1+\lambda w_2 \in W$.
\end{remark}
\subsection{The lattice of subspaces}
\begin{proposition}
	Let V be an $F$-vector space. Let $\{S_\alpha: \alpha \in I\}$ be a collection of subspaces of $V$. Then $X := \bigcap_{\alpha \in I}S_\alpha$ is also a subspace.
\end{proposition}
\begin{proof}
	Note that $0_V \in S_\alpha$ for all $\alpha \in I$, i.e. $0_V \in X$. 
	Pick $v, w \in X$, $\lambda, \mu \in F$. Then $v, w \in S_\alpha$ for all $\alpha \in I$. By the definition of subspace, $\lambda v + \mu w \in X$. 
\end{proof}
Let $\mathcal{S}(V)$ be collection of subspaces of $V$. We define a partial order on $\mathcal{S}$: $W_1 \le W_2 \iff W_1 \subset W_2$. 
\begin{remark}
	Given $W_1, W_2 \in \mathcal{S}(V)$, there is always a `bigger' element in $\mathcal{S}(V)$ that is smaller than both $W_1$, $W_2$ (analogous to gcd) which is $W_1 \cap W_2$. 
\end{remark}
We now explore the analogy to lcm in $\mathcal{S}(V)$. 
\begin{proposition}
	\begin{enumerate}
		\item Let $W_1, W_2 \in \mathcal{S}(V)$. Then $W_1 \cup W_2 \in \mathcal{S}(V) \iff W_1 \subset W_2 \text{ or } W_2 \subset W_1$.
		\item If $F$ is infinite, $V$ can never be the union of a finite number of proper subspaces. 
	\end{enumerate}
\end{proposition}
\begin{proof}
	\begin{enumerate}
		\item The $\Leftarrow$ direction is obvious. Now suppose $W_1$ and $W_2$ are not subsets of each other. Then we can pick $u_1 \in W_2 - W_1$, $u_2 \in W_1 - W_2$. Let $v= u_1+u_2 \in W_1 \cup W_2$. We get contradiction by considering $v-u_1$ and $v-u_2$. 
		\item We induct on the number of proper subspaces in the union. The base case is obvious. 
		Consider $U_1 \cup U_2 \cup \dots \cup U_n$ to be union of proper subspaces of $V$. We can pick $u_1 \in U_1$ but $u_1 \notin U_i$ for all $i>1$ (otherwise, we are reduced to the induction hypothesis), and pick $v \in V - U_1$. Then consider vectors of the form $v + \lambda u_1$ which cannot be in $U_1$. Furthermore, for $\lambda_1 \ne \lambda_2$, $v + \lambda_1 u_1 - (v + \lambda_2 u_1) = (\lambda_1-\lambda_2)u_1 \notin U_i$ for all $i>1$. Because $F$ is infinite, there exists $\lambda_0$ such that $v + \lambda_0 u_1 \notin U_1 \cup U_2 \cup \dots \cup U_n$. 
	\end{enumerate}
\end{proof}
\begin{definition}
	Let $U, W \in \mathcal{S}(V)$; 
	\[U+W := \{u+w: u\in U, w\in W\}.\]
	We define $U_1+U_2+\dots+U_n$ similarly. 
\end{definition}
\begin{proposition}
	$U+W$ is a vector subspace, and it is the smallest subspace containing both $U$ and $W$. Similarly, $U_1+U_2+\dots+U_n$ is the smallest subspace containing $U_i$ for all $1\le i\le n$.
\end{proposition}
\begin{proof}
	Note that 
	\begin{enumerate}
		\item $0 = 0_V = 0_V+0_V \in U + W$
		\item let $v_1, v_2 \in U+W$; $\lambda, \mu \in F$. Then 
		\begin{align*}
			\lambda v_1 + \mu v_2 &= \lambda(u_1+w_1) + \mu(u_2+w_2) \\
			&= (\lambda u_1+\mu u_2) + (\lambda w_1+\mu w_2) \\
			&\in U+W
		\end{align*}
		\item $U+0$ and $0+W$ and contained in $U+W$. 
		\item Suppose $V'$ is the smallest subspace containing $U$ and $W$. Then clearly $U+W \subset V'$, so $V' = U+W$ by definition of $V'$. 
	\end{enumerate}
\end{proof}
\begin{proposition}
	$U_1, \dots, U_n \in \mathcal{S}(V)$. The following are equivalent:
	\begin{enumerate}
		\item whenever $\sum_{i=1}^n u_i = \sum_{i=1}^n u'_i$ for $u_i, u'_i \in U_i$, we must have $u_i = u'_i$ for all $i$.
		\item whenever $\sum_{i=1}^n u_i = 0$ for $u_i \in U_i$, we must have $u_i = 0$ for all $i$.
		\item $(\sum_{i\ne j} U_i) \cap U_j = \{0\}$ for all $j$.
	\end{enumerate}
\end{proposition}
\begin{definition}
	We say that the sum $\sum_{i=1}^n U_i$ is a direct sum and write $\bigoplus_{i=1}^n U_i$ whenever the conditions in the previous proposition are met. 
\end{definition}
\begin{example}
	$F = \R$, $V=\R^3$; let
	\begin{align*}
		&U_1 = \{(x, 0, 0): x\in \R\} \\
		&U_2 = \{(0, y, 0): y\in \R\}\\
		&U_3 = \{(0, 0, z): z\in \R\}\\
		&U_4 = \{(a, a, 0): a\in \R\}.
	\end{align*}
	$U_1+U_2+U_3$ is a direct sum; $U_1+U_2+U_4$ is not. 
\end{example}
\begin{definition}
	For an arbitrary set $I$, suppose $\{U_i: i\in I\}$ is a collection of subspaces of $V$. 
	\[\sum_{i \in I}U_i = \bigcup_{\substack{A \subset I \\ |A| < \infty}} \sum_{\alpha \in A}U_\alpha\]
\end{definition}
\begin{example}
	Any matrix $A \in M_n{R}$, where $R$ is a ring in which $2$ is a unit. Then we can write
	\[A = \frac{1}{2}(A+A^T) + \frac{1}{2}(A-A^T)\]
	i.e. $A$ is a sum of a symmetric and a skew-symmetric matrix. One can check that $\text{Sym}_n$ and $\text{Skew-Sym}_n$ are subpaces, and furthermore $M_n(R) = \text{Sym}_n \oplus \text{Skew-Sym}_n$. 
\end{example}
\subsection{Span and linear independence}
\begin{definition}
	Let $V$ be an $F-$vector space. Let $S \subset V$. The subspace spanned by $S$
	\[\spn(S) = \langle S \rangle:= \{r_1v_1+\dots+r_nv_n: r_i \in F, v_i \in S\}.\]
	We say that $r_1v_1+\dots+r_nv_n$ is a linear combination of $v_1, dots, v_n$. If $V = \spn(S)$, then we say that $S$ is a spanning set of $V$.
\end{definition}
\begin{remark}
	Any superset of a spanning set is also a spanning set. 
\end{remark}
\begin{definition}
	A nonempty set $S \subset V$ is linearly independent if for any $v_1, \dots, v_n \in S$, $r_1, \dots, r_n \in F$, we have
	\[\sum_{i=1}^n r_iv_i=0 \implies r_1=r_2=\dots=r_n=0.\]
\end{definition}
\begin{remark}
	\begin{enumerate}
		\item If $S$ is linearly independent, then $0 \notin S$.
		\item Any nonempty subset of a linearly independent set is linearly independent.
	\end{enumerate}
\end{remark}
\begin{lemma}
	Let $S \subset V$; the following are equivalent:
	\begin{enumerate}
		\item $S$ is linearly independent
		\item every vector in $\spn(S)$ is a unique linear combination of vectors in $S$
		\item no vector in $S$ is a linear combination of other vectors in $S$, i.e. for all $v \in S$, $v \notin \spn(S-\{v\})$.
	\end{enumerate}
\end{lemma}
\begin{theorem}
	Let $S \subset V$; the following are equivalent:
	\begin{enumerate}
		\item $S$ linearly independent and spans $V$
		\item For each vector $v\in V$, there exists unique $v_1, \dots, v_n \in S$ and $r_1, \dots, r_n \in F$ such that
		\[v=\sum_{i=1}^n r_iv_i.\]
		\item $S$ spans $V$ but any proper subset of $S$ does not span $V$
		\item $S$ is linearly independent but any proper superset of $S$ is not linearly independent.
	\end{enumerate}
\end{theorem}
\iffalse
\begin{proof}
	It is clearly that 1. and 2. are equivalent. 
	\par $(i) \implies (iii)$: Suppose $S' \subsetneq S$ is a spanning set of $V$. Then any vector in $S-S'$ can be written as a linear combination of vectors in $S'$. Pick $v \in S-S'$, then
	\[v = \sum_{i=1}^n a'_i v'_i\]
	where $v'_i \in S' \subsetneq S$, contradicting the linear independence of $S$.
	\par $(iii) \implies (i)$: Suppose $S$ is not linearly independent. Then we can find $v\in S$ such that $v \in \spn(S-\{v\})$, which implies that
	\[V = \spn{S} = \spn(S-\{v\}), \]
	so we has found a proper subset of $S$ that spans $V$.
\end{proof}
\fi
\begin{definition}
	Let $S \subset V$; We say that $S$ is a basis of $V$ if it satisfies any of the equivalent criteria above. 
\end{definition}
\begin{corollary}
	A finite set $S = \{v_1, \dots, v_n\}$ is a basis of $V$ if and only if
	\[V = \spn(v_1) \oplus \spn(v_2) \oplus \dots \oplus \spn(v_n).\]
\end{corollary}
\begin{example}
	Let $V = F^n = \{(a_1, \dots a_n): a_i \in F\}$. Then $S = \{e_1, \dots, e_n\}$ is a basis of $V$.
\end{example}
\begin{remark}
	The zero vector space $\{0\}$ does not have a basis. 
\end{remark}
\begin{theorem}
	Let $V$ be a non-zero F-vector space. Let $I$ be a linearly independent set in $V$, and $S$ is a spanning set of $V$ containing $I$. Then there exists a basis $\B$ such that such that $I \subset B \subset S$. In particular:
	\begin{enumerate}
		\item every non-zero vector space has a basis
		\item every linearly independent set can be extended to be a basis
		\item every spanning set can be shrunk to be a basis.
	\end{enumerate}
\end{theorem}
\begin{lemma}[Zorn's lemma]
	Let $S$ be a poset. If every totally ordered subset (chain) of $S$ has an upper bound, then $S$ has a maximal element. 
\end{lemma}
\begin{proof}[Proof of theorem]
	Consider 
	\[\mathcal{P} := \{\Lambda \subset V: I \subset \Lambda \subset S,\, \text{$\Lambda$ is linearly independent}\}.\]
	Observe that $\mathcal{P}$ is nonempty ($I \in \mathcal{P}$). We claim that if $\mathcal{C} = \{I_k: k \in K\}$ is chain in $\mathcal{P}$, then $U = \bigcup_{k \in K}I_k$ is an upper bound in $\mathcal{P}$ (in particular, it is in $\mathcal{P}$). Indeed, we check that firstly, since $I \subset I_k \subset S$, we have $I \subset U \subset S$. Secondly, $U$ is linearly independent, because given $v_1, \dots, v_l \in U$ such that $\sum_{i=1}^l c_iv_i = 0$, then since $\mathcal{C}$ is a chain, we can find some $I_m$ which contains all $v_1, \dots, v_l$. Then we get $c_1=\dots=c_l=0$ by linear independence of $I_m$.
	\par By Zorn's lemma, there exists $\mathcal{B} \in \mathcal{P}$ which is maximal. We now claim that $\mathcal{B}$ is a basis. Indeed, $\mathcal{B}$ is linearly independent. Also, suppose $v \in V$ is such that $v \notin \spn{\mathcal{B}}$. Then $\mathcal{B} \cup \{v\}$ is linearly independent. This contradicts the maximality of $\mathcal{B}$. So $\mathcal{B}$ is also spanning.
\end{proof}
\begin{remark}
	One might try to argue with
	\[\mathcal{P}' := \{\Lambda \subset V: I \subset \Lambda \subset S,\,\text{$\Lambda$ is spanning}\}.\]
	One then need to check that if $\{I_k: k\in K\}$ are all spanning, then $\bigcap I_k$ is also spanning. Alas, this is not true. For example, $V=\Q$, $F=\Q$, $S = \Q = \{r_1, r_2, \dots\}$. Let $I_k := S - \{r_1, \dots, r_k\}$. Then $I_k$ are all spanning, but $\bigcap I_k = \emptyset$.
\end{remark}
\begin{definition}
	Let $V$ be a vector space. If there is a basis of $V$ consisting of finitely many elements, we say that $V$ is finite-dimensional. Otherwise, we say that $V$ is infinite dimensional.
\end{definition}
\begin{theorem}
	Let $V$ be an $F-$vector space; assume $\{v_1, \dots, v_n\}$ is a linearly independent set and $\{s_1, \dots, s_m\}$ is a spanning set for $V$, then $n \le m$.
\end{theorem}
\begin{proof}
	Consider listing the vectors in the following way, where the left side is always spanning:
	\[s_1, s_2, \dots, s_m; v_1, v_2, \dots, v_n\]
	\[v_1, s_1, s_2, \dots, s_m; v_2, \dots, v_n.\]
	Since $v_1$ is a linearly combinations of $s_i$'s, one of the $s_i$'s, say $s_1$, can be written using $\{v_1, s_2, s_3, \dots, s_m\}$. 
	\[v_1, s_2, \dots, s_m; v_2, \dots, v_n.\]
	We can repeat the argument and get (wlog)
	\[v_1, v_2, \dots, s_m; v_3, \dots, v_n.\]
	Suppose $n>m$, then eventually we get
	\[v_1, v_2, \dots, v_m; v_{m+1}, \dots, v_n.\]
	Now the left side is still spanning, contradicting the assumption that $\{v_1, \dots, v_n\}$ is linearly independent. 
\end{proof}
\begin{corollary}
	If $V$ is finite-dimensional, and $\B_1, \B_2$ are bases of $V$, then $|\B_1| = |\B_2|$. 
\end{corollary}
\begin{definition}
	The dimension of a finite-dimensional vector space $V$ is the number of elements in its basis.
\end{definition}
\begin{proposition}
	Let $S, T$ be two subspaces of a finite-dimensional vector space $V$. Then 
	\[\dim(S+T) = \dim(S)+\dim(T)-\dim(S \cap T).\]
	In particular, if $S$ is a complement of $T$ (i.e. $S + T = V$ and $S \cap T = \{0\}$), then $V = S \oplus T$ and 
	\[\dim V = \dim(S)+\dim(T).\]
\end{proposition}
\begin{proof}
	Pick a basis $\B = \{b_i\}$ of $S \cap T$. Then we can extend $\B$ to a basis $\B \cup \mathcal{A}$ of $S$ and extend $\B$ to a basis $\B \cup \mathcal{C}$ of $T$. It is not hard to check that $\mathcal{A} \cup \B \cup \mathcal{C}$ is a basis of $S+T$. Then 
	\begin{align*}
		\dim{S}+\dim{T} &= |\B \cup \mathcal{A}| + |\B \cup \mathcal{C}| \\
		&= |\mathcal{A}|+|\mathcal{C}|+2|B| \\
		&= |\mathcal{A}|+|\mathcal{B}|+|\mathcal{C}|+|\B| \\
		&= \dim(S+T)+\dim(S \cap T).
	\end{align*}
\end{proof}
\section{Linear maps}
\subsection{Linear maps and matrices}
Let $U, V$ be $F-$vector spaces. We want to understand linear maps $U \to V$. 
\begin{definition}
	A map $\alpha: U \to V$ is linear if for all $u, u' \in U$, $\lambda \in F$, 
	\begin{enumerate}
		\item $\alpha(u+u') = \alpha(u)+\alpha(u')$
		\item $\alpha(\lambda u) = \lambda \alpha(u)$.
	\end{enumerate}
\end{definition}
\begin{example}
	\begin{enumerate}
		\item $\id = \id_V: V \to V$; $c\id_V$; zero map $V\to V$
		\item For $A \in M_{m \times n}(F)$, we have
		\begin{align*}
			\mathcal{L}_A: F^n &\to F^m \\
			x &\mapsto Ax
		\end{align*}
		\item $D^\infty(\R) := \text{infinitely many times differentiable functions}$
		\begin{align*}
			F: D^\infty(\R) &\to D^\infty(\R) \\
			f &\mapsto f'
		\end{align*}
		\item $C^\infty(\R) := \text{continuous functions $\R \to \R$}$
		\[f \mapsto \left(x \mapsto \int_0^x f(t)\,dt\right)\]
		\item \begin{align*}
			f: F &\to F \\
			x &\mapsto x^2
		\end{align*}
		where $\charf{F}=2$. 
	\end{enumerate}
\end{example}
We use the notation $\L(V, W)$ to denote the set of all linear transformations $V \to W$. 
\begin{lemma} \label{uniquematrix}
	Let $\alpha \in \L(F^n, F^m)$. Then there exsits a unique $A \in M_{m \times n}(F)$ such that $\alpha = \L_\alpha$ (refer to the previous example for the notation).
\end{lemma}
\begin{proof}
	The $i$-th column of $A$ is $\alpha(e_i^{(n)})$. 
\end{proof}
\begin{remark}
	Let $U, V$ be vector spaces, let $\B$ be a basis of $U$. Pick $\{v_b \in V: b\in \B\}$. Then the map
	\begin{align*}
		\alpha: U &\to V \\
		\sum_{b\in \B} \lambda_b b &\mapsto \sum_{b\in \B} \lambda_b v_b
	\end{align*}
	is the unique linear transformation such that $\alpha(b) = v_b$ for all $b\in \B$. 
\end{remark}
\begin{proposition}
	Let $S$ be a nonempty set and $V, W$ be vector spaces. Then $\text{Fun}(S, W)$ is a vector space. In particular $\L(V, W)$ is a vector space.
\end{proposition}
\begin{lemma}
	Composition of linear transformations is a linear transformation. 
\end{lemma}
\begin{definition}
	Let $\alpha \in \L(U, V)$. 
	\begin{enumerate}
		\item $\ker{\alpha} := \{u \in U: \alpha(u)=0_V\} = \alpha^{-1}(0_V)$
		\item $\im{\alpha} := \{\alpha(u): u \in U\} \subset V$. 
	\end{enumerate}
\end{definition}
\begin{lemma}
	\begin{enumerate}
		\item $\ker{\alpha}$ is a subspace of $U$.
		\item $\im{\alpha}$ is a subspace of $V$.
	\end{enumerate}
\end{lemma}
\begin{theorem}[rank-nullity theorem]
	Let $\alpha \in \L(U, V)$, $\dim{U} < \infty$. Then 
	\[\dim(\im{\alpha})+\dim(\ker{\alpha}) = \dim{U}.\]
\end{theorem}
\begin{proof}
	Pick a basis $\{u_1, \dots, u_k\}$ of $\ker(\alpha) \subset U$. We can extend this basis to a basis $\{u_1, \dots, u_k, u_{k+1}, \dots, u_n\} \subset U$ of $U$. We claim that $\{\alpha(u_{k+1}), \dots, \alpha(u_n)\}$ is a basis of $\im(\alpha)$. Indeed, 
	\begin{align*}
		&\sum_{i=k+1}^n c_i \alpha(u_i) = \alpha(\sum_{i=k+1}^n c_i u_i) = 0_V \\
		\implies &\sum_{i=k+1}^n c_i u_i \in \ker{\alpha} \\
		\implies &\sum_{i=k+1}^n c_i u_i = \sum_{i=1}^k d_i u_i \\
		\implies &c_i = 0 \, \forall i,
	\end{align*}
	showing linear independence. Moreover, let $v \in \im{\alpha}$, then 
	\[v = \alpha\left(\sum_{i=1}^n c_i u_i\right) = \alpha\left(\sum_{i=1}^k c_i u_i\right)+\sum_{i=k+1}^n c_i \alpha(u_i) = \sum_{i=k+1}^n c_i \alpha(u_i).\]
	Hence $\{\alpha(u_{k+1}), \dots, \alpha(u_n)\}$ spans $\im{\alpha}$.
\end{proof}
\begin{remark}
	This is a consequence of the first isomorphism theorem for vector spaces:
	\[\begin{tikzcd}
		V &&&& W \\
		\\
		&& {V/\ker{\alpha}}
		\arrow["\alpha", from=1-1, to=1-5]
		\arrow["\pi"', two heads, from=1-1, to=3-3]
		\arrow["\iota"', hook, from=3-3, to=1-5]
	\end{tikzcd}\]
	where $\pi: V \to V/\ker{\alpha}$ is the quotient map $\pi(v) = v+\ker{\alpha}$. 
	We define $\iota: V/\ker{\alpha} \to  W$ by $\iota(v+\ker{\alpha}) = \alpha(v)$ (check that this is well-defined!). Hence, the diagram is commutative. The first isomorphism theorem says that $V/\ker{\alpha} \cong \im{\alpha}$. Now assuming everything is finite-dimensional, we get 
	\[\dim{V}-\dim{\ker{\alpha}} = \dim{V/\ker{\alpha}} = \dim{\im{\alpha}}\]
	which is the rank-nullity theorem. The advantage of this approach is that it makes sense even in the infinite-dimensional case. 
\end{remark}
\begin{corollary}
	Let $\alpha \in \L(U, V)$. The following are equivalent:
	\begin{enumerate}
		\item $\alpha$ injective
		\item $\ker{\alpha} = \{0_U\}$
		\item $\dim \ker {\alpha} = 0$. 
	\end{enumerate}
\end{corollary}
\begin{proposition}
	Let $\alpha \in \L(U, V)$, $\dim U = \dim V < \infty$. Then $\alpha$ is injective if and only if it is surjective.
\end{proposition}
\begin{proof}
	\[\alpha \text{ injective} \iff \dim \ker {\alpha} = 0 \iff \dim \im{\alpha} = \dim U = \dim V \iff \alpha \text{ surjective}.\]
\end{proof}
Let $V$ be an $F-$vector space such that $\dim{V} = n < \infty$. We want to understand $\L(V, V)$, i.e. $M_{n \times n}(F)$. In particular, we can consider the trace: 
\begin{align*}
	M_{n \times n}(F) &\to F \\
	(a_{ij}) &\mapsto \sum_{i=1}^n a_{ii}.
\end{align*}
We would like to prove that things like this are indepedent of the choice of basis. In fact, we would see later that $\L(V, V) \cong V^* \otimes V$ and the trace can be defined as 
\begin{align*}
	V^* \otimes V &\to F \\
	f \otimes v &\mapsto f(v).
\end{align*}
This definition is basis-free, and even generalises to infinite-dimensional case with some restriction. This is one motivation for our subsequent discussions. 
\begin{definition}
	An isomorphism is a bijective linear map. 
\end{definition}
\begin{lemma}
	$V \cong W \iff |\B_V| = |\B_W|$ where $\B_V, \B_W$ are bases of $V$ and $W$ respectively. 
\end{lemma}
\begin{proof}
	($\Rightarrow$) Pick $T: V \cong W$. Then $\{T(v): v \in \B_V\}$ is a basis of $W$. 
	\par ($\Leftarrow$) Given a bijection $\alpha: \B_V \to \B_W$, extend $\alpha$ to a linear map $T: V \to W$. It is clear that $T$ is bijective. 
\end{proof}
\begin{definition}
	Let $V$ be a finite-dimensional $F-$vector space, $\B = \{v_1, \dots, v_n\}$ be an ordered basis. For $v = \sum_{i=1}^n \lambda_i v_i \in V$, define $[v]_\B := (\lambda_1, \lambda_2, \dots, \lambda_n)^T$.
\end{definition}
\begin{lemma}
	Fix an ordered basis $\B$ of $V$. If $\dim{V} = n < \infty$, then
	\begin{align*}
		\iota_\B:V &\to F^n \\
		v &\mapsto [v]_\B
	\end{align*}
	is an isomorphism. 
\end{lemma}
\begin{definition}
	Let $V$ and $W$ be finite-dimensional vector spaces. Let $\alpha \in \L(V, W)$. Fix $\B_V = \{v_1, \dots, v_n\}, \B_W = \{w_1, \dots, w_m\}$. Define
	\[[\alpha]_{\B_W, \B_V} := 	\begin{pmatrix}
		[\alpha(v_1)]_{\B_W} & [\alpha(v_2)]_{\B_W} &\dots & [\alpha(v_n)]_{\B_W}
	\end{pmatrix} \in M_{m \times n}(F).\]
\end{definition}
\begin{lemma}
	Let $V$ and $W$ be finite-dimensional vector spaces. Let $\alpha \in \L(V, W)$. Fix ordered bases $\B_V = \{v_1, \dots, v_n\}, \B_W = \{w_1, \dots, w_m\}$.  
	\begin{enumerate}
		\item 	there is a unique $A \in M_{m \times n}(F)$ such that $[\alpha(v)]_{\B_W} = A[v]_{\B_V}$ for all $v \in V$. In fact, $A = [\alpha]_{\B_W, \B_V}$.
		\item 	\begin{align*}
			\L(V, W) &\to M_{m \times n}(F) \\
			\alpha &\mapsto [\alpha]_{\B_W, \B_V}
		\end{align*}
		is an isomorphism. 
	\end{enumerate}
\end{lemma}
\begin{proof}
	\begin{enumerate}
		\item Our choices of basis induce isomorphisms $\iota_{\B_V}: V \to F^n$ and $\iota_{\B_W}: W \to F^m$. By Lemma \ref{uniquematrix}, there is a unique $A \in M_{m \times n}(F)$ such that $\L_A = \iota_{\B_W}\circ\alpha\circ \iota_{\B_V}^{-1}$, namely the matrix whose $i-$th column is $\iota_{\B_W}\circ\alpha \circ\iota_{\B_V}^{-1}(e^{(n)}_i) = [\alpha(v_i)]_{\B_W}$. Hence $A$ is precisely $[\alpha]_{\B_W, \B_V}$. 
		\item The map is clearly linear. Moreover, because $[\alpha]_{\B_W, \B_V}$ encodes $\alpha(v_i)$ (for all $i$), there is an inverse. 
	\end{enumerate}
\end{proof}
\begin{corollary}
	Let $V, W, X$ be finite-dimensional. Let $\alpha \in \L(V, W), \beta \in \L(W, X)$. Pick ordered bases $\B_V, \B_W, \B_X$. We have
	\[[\beta\circ\alpha]_{\B_X, \B_V} = [\beta]_{\B_X, \B_W}[\alpha]_{\B_W, \B_V}.\]
\end{corollary}
\begin{proof}
	For all $v \in V$
	\begin{align*}
		[\beta \circ \alpha]_{\B_X, \B_V}[v]_{\B_V} &= [(\beta \circ \alpha)(v)]_{\B_X} \\
		&= [\beta(\alpha(v))]_{\B_X} \\
		&= [\beta]_{\B_X, \B_W}[\alpha(v)]_{\B_W} \\
		&= [\beta]_{\B_X, \B_W}[\alpha]_{\B_W, \B_V}[v]_{\B_V}
	\end{align*}
	\[\implies [\beta \circ \alpha]_{\B_X, \B_V} = [\beta]_{\B_X, \B_W}[\alpha]_{\B_W, \B_V}.\]
\end{proof}
\begin{proposition}
	Let $V$ and $W$ be finite-dimensional vector spaces. Let $\alpha \in \L(V, W)$. There exists ordered bases $\B_V = \{v_1, \dots, v_n\}, \B_W = \{w_1, \dots, w_m\}$ such that if $[\alpha]_{\B_W, \B_V} = (a_{ij})$, then 
	\[a_{ij} = \left\{
		\begin{aligned}
			&1, \,i=j=1, 2, \dots, \rank{\alpha} \\
			&0, \,\text{otherwise}
		\end{aligned}
		\right.\]
\end{proposition}
\begin{proof}
	Pick a basis $\{v_{r+1}, \dots, v_n\}$ of $\ker{\alpha}$. Extend to a basis $\{v_1, \dots, v_r, v_{r+1}, \dots ,v_n\}$ of $V$. Then $\{\alpha(v_1), \dots, \alpha(v_r)\}$ is a basis of $\im{\alpha}$. Then we extend this to a basis $\{w_1, \dots, w_r, w_{r+1}, \dots ,w_m\}$ of $W$. Then $\rank(\alpha)=r$ and $[\alpha]_{\B_W, \B_V}$ is of the desired form. 
\end{proof}
\subsection{Dual spaces}
\begin{definition}
	Let $V$ be an $F-$vector space. We define the dual space of $V$, $V^* := \L(V, F)$; elements of $V^*$ are called linear functionals.
\end{definition}
\begin{example}
	\begin{enumerate}
		\item \begin{align*}
			\text{ev}_a: V = F[x] &\to F \\
			f &\mapsto f(a)
		\end{align*}
		\item \begin{align*}
			\text{Trace}: V = M_{n \times n}(F) &\to F \\
			(a_{ij}) &\mapsto \sum_{i=1}^n a_{ii}.
		\end{align*}
	\end{enumerate}
\end{example}
\begin{remark}
	Let $\alpha: V \to F$. Note that $\rank{\alpha}$ is either $0$ ($\alpha$ is identically zero) or $1$. Applying rank-nullity, we have unless $\alpha=0$, $\dim \ker \alpha = \dim{V}-1$.
\end{remark}
\begin{proposition} \label{dualstuff}
	\begin{enumerate}
		\item For all $v \in V-\{0\}$, there exists $f \in V^*$ such that $f(v) \ne 0$
		\item $v \in V$ is zero vector if and only if $f(v)=0$ for all $f \in V^*$
		\item If $f(v) \ne 0$, then $V = \spn(v) \oplus \ker{f}$
		\item if $f, g \in V^*$, both non-zero, then $\ker{f} = \ker {g}$ if and only if $f = \lambda g$ for some $\lambda \in F$.
	\end{enumerate}
\end{proposition}
\begin{proof}
	\begin{enumerate}
		\item Extend $\{v\}$ to a basis, then define $f(v) \ne 0$. 
		\item Because $f$ is a linear functional, $f(0)=0$. The converse follows from above. 
		\item Pick $x \in \spn(v) \cap \ker{f}$, then $x = av$ for some $a \in F$ and $f(x)=0$. So $f(av) = af(v)=0$, implying $x = 0$. Hence $\spn(v) \cap \ker{f} = \{0\}$. Moreover, let $x \in V$. Then 
		\[x = x-\frac{f(x)}{f(v)}v+\frac{f(x)}{f(v)}v\]
		and we can check that $f\left(x-\frac{f(x)}{f(v)}v\right) = f(x)-\frac{f(x)}{f(v)}f(v) = 0$.
		\item If $f = \lambda g$, then obviously $\ker{f}=\ker{g}$. Conversely, suppose $\ker{f}=\ker{g} =: K$. Pick $x \notin K$, by above, $V = \spn(x) \oplus K$. Of course, $f\vert_K = \lambda g\vert_K$ for any $\lambda \in F$. Then let $\lambda := \frac{f(x)}{g(x)}$ to ensure that 
		\[\lambda g(cx) = c\frac{f(x)}{g(x)}g(x) = cf(x), \]
		so $f= \lambda g$ on the whole of $V$, by the previous part. 
	\end{enumerate}
\end{proof}
\begin{definition}[Dual basis]
	Let $\B = \{v_i: i \in I\}$ be a basis of $V$. Define $v_i^* \in V^*$ by setting $v_i^* = \delta_{v_i}$.
\end{definition}
\begin{theorem}
	\begin{enumerate}
		\item $\B^* = \{v_i^*: i \in I\}$ is linearly independent (in $V^*$)
		\item If $V$ is finite-dimensional, then $\B^*$ is also spanning and hence is a basis (thus justifying the term `dual basis')
	\end{enumerate}
\end{theorem}
\begin{proof}
	\begin{enumerate}
		\item Suppose $\sum_{i=1}^n a_iv_i^* =0$ in $V^*$. Then evaluating on $v_i$ gives $a_i = 0$ for all $i$. 
		\item For any $f \in V^*$, check that $f = \sum_{i\in I} f(v_i)v_i^*$ (this is a finite sum!) by evaluating both sides at $v_i$'s. 
	\end{enumerate}
\end{proof}
\begin{remark}
	
\end{remark}
\begin{corollary} \label{dualiso}
	When $V$ is finite-dimensional, $\dim V^* = \dim V$, so $V^* \cong V$. 
\end{corollary}
Note that Corollary \ref{dualiso} is false when $\dim V = \infty$: 
\begin{example}
	Let $V$ be any infinite-dimensional vector space over $\Z_2$ with a countable basis $\B$. Any vector $v \in V$ is an indication of a finite subset of $\B$, i.e. there is an injection from $V$ to the set of finite subsets of $\B$, which is countable. So $V$ is (at most) countable. However, there is a surjection $V^* \to \mathcal{P}(\B)$. So $V^*$ has strictly greater cardinality than $V$. 
\end{example}
We can take dual of the dual space $V^*$. This gives rise to the double dual $V^{**} = (V^*)^* = \L(V^*, F)$. Unlike for $V^*$, there is a canonical/natural isomorphism $V \cong V^{**}$ when $\dim{V}<\infty$: define 
\begin{align*}
	\text{ev}: V &\to V^{**} \\
	v &\mapsto (T \mapsto T(v)).
\end{align*}
One can check that this is a linear map. 
\begin{proposition}
	The above $\text{ev}: V \to V^{**}$ is an isomorphism when $\dim V < \infty$. 
\end{proposition}
\begin{proof}
	It suffices to show that $\text{ev}$ is injective. Let $v \in \ker{\text{ev}}$, then $\text{ev}(v) \in \L(V^*, F)$ is the zero map, i.e. $\text{ev}(f)=0$ for all $f \in V^*$. By Proposition \ref{dualstuff}, $v = 0$. 
\end{proof}
\subsection{Invariant subspaces}
We write $\L(V) := \L(V, V)$. For $\alpha \in \L(V)$, we abbreviate $[\alpha]_{B_V, B_V}$ as $[\alpha]_{B_V}$. 
\begin{definition}
	Let $\alpha \in \L(V)$, $W \subset V$ a subspace. We say that $W$ is $\alpha-$invariant if $\alpha(W) \subset W$, i.e. $\alpha\vert_W$ is in $\L(W)$. 
\end{definition}
\begin{example}
	\begin{enumerate}
		\item $\ker{\alpha}, \im{\alpha}$ are always $\alpha$-invariant.
		\item Let $v \in V$. Then $\spn\{\alpha^i(v)\}_{i \in \Z_{>0}}$ is the smallest $\alpha-$invariant subspace containing $v$. 
	\end{enumerate}
\end{example}
\begin{lemma}
	Let $\alpha \in \L(v)$, $\dim V < \infty$; suppose $V = U\oplus W$ and $U, W$ are $\alpha-$invariant. Pick ordered basis $\B_V = \B_U \cup \B_W$. Then
	\[[\alpha]_{\B_V} = \begin{pmatrix}
		[\alpha]_{\B_U} & \\
		& [\alpha]_{\B_W}
	\end{pmatrix}.\]
\end{lemma}
\subsection{The minimal polynomial}
We observe that $\L(V)$ can be considered as a non-commutative ring (with composition as the ring multiplication). Let $p = \sum_{i=0}^{\deg{p}}a_ix^i \in F[x]$ and $\alpha \in \L(V)$. Then $p(\alpha) = \sum a_i \alpha^i \in \L(V)$. 
\begin{remark}
	\begin{enumerate}
		\item $\alpha \beta = \beta \alpha \implies p(\alpha)q(\beta) = q(\beta)p(\alpha)$
		\item $\spn\{\alpha^i(v)\}_{i \in \Z_{>0}} = \{p(\alpha)v: p \in F[x]\}$
	\end{enumerate}
\end{remark}
\begin{definition}
	Let $\alpha \in \L(V)$, $p \in F[x]-\{0\}$. Then we say that $\alpha$ satisfies $p(x)$ if $p(\alpha) = 0$ in $\L(V)$. 
\end{definition}
\begin{lemma}
	If $\dim{V} < \infty$, then there exists $p(x) \in F[x]$ with $\deg{p} \le n^2$ such that $\alpha$ satisfies $p(x)$.
\end{lemma}
\begin{proof}
	The collection $\{1, \alpha, \alpha^2, \dots, \alpha^{n^2}\} \subset \L(V)$ has $n^2+1 > \dim{L(V)}$ elements, so it is not linearly indepedent.
\end{proof}
\begin{remark}
	The above fails if $\dim{V} = \infty$. Consider `shift' in $F[x]$. This shows that there is no minimal polynomial in this case.
\end{remark}
\begin{definition}
	Let $\alpha \in \L(V)$. The minimal polynomial of $\alpha$, denoted $m_\alpha(x) \in F[x]$, is some polynomial (if exists) such that:
	\begin{enumerate}
		\item $m_\alpha(\alpha) = 0$
		\item if $p(x) \in F[x]-\{0\}$ is such that $p(\alpha) = 0$, then $\deg{p} \ge \deg{m_\alpha}$
		\item $m_\alpha$ is monic.
	\end{enumerate}
\end{definition}
\begin{lemma}
	Let $\alpha \in \L(V)$; suppose $\alpha$ satistifies some non-zero polynomial. Then $m_\alpha$ exsists and is unique. Moreover, $\alpha$ satistifes $p(x)$ if and only if $m_\alpha \mid p(x)$.
\end{lemma}
\begin{proof}
	Define $X_\alpha := \{p(x) \in F[x]: p(\alpha)\}$. By assumption $X_\alpha \ne \emptyset$. Then we can define $m_\alpha = \text{a monic polynomial in $X_\alpha$ having minimal degree}$. Moreover, we can write $p(x) = q(x)m_\alpha(x)+r(x)$ where $r(x) = 0$ or $\deg{r} < \deg{m_\alpha}$. Now if $p(\alpha) = 0$, then $r(\alpha) = 0$, so $r(x) = 0$ by minimality of $m_\alpha$. Conversely, if $m_\alpha \mid p(x)$, then clearly $\alpha$ satistifes $p(x)$. Finally, it is then clear that $m_\alpha$ is unique. 
\end{proof}
\begin{remark}
	Notice that the above argument does not use the linearity of $\alpha$. In fact, observe that $X_\alpha \subset F[x]$ is an ideal. So this argument is essentially showing that any Euclidean domain is a principal ideal domain. In this case, $m_\alpha$ is the generator of the ideal $X_\alpha$.
\end{remark}
\begin{corollary} \label{dividem}
	Let $\alpha \in \L(V)$ with a minimal polynomial $m_\alpha$. Suppose $U \subset V$ is $\alpha$-invariant. Then $m_{\alpha \vert_U} \mid m_\alpha$ and $m_{\tilde{\alpha}} \mid m_\alpha$, where $\tilde{\alpha}: V/W \to V/W$, $\tilde{\alpha}(v+U) = \alpha(v)+U$.
\end{corollary}
\begin{proof}
	Note that $m_\alpha(\alpha \vert_U)=0$, so $m_{\alpha \vert_U} \mid m_\alpha$ by minimality of $m_{\alpha \vert_U}$. Similarly, $m_\alpha(\tilde{\alpha})(v+U) = m_\alpha(\alpha)v+U = 0+U$. 
\end{proof}
\begin{corollary}
	Let $\alpha \in \L(V)$; $W_1, \dots, W_k \subset V$ are $\alpha$-invariant. Set $m_i(x) = m_{A \vert_{W_i}}(x)$; if $\sum W_i = V$, then $m_\alpha(x) = \lcm(m_1, \dots, m_k)$.
\end{corollary}
\begin{proof}
	We know that $m_i \mid m_\alpha$ for all $i$. Moreover, suppose $m \in F[x]$ is a common multiple of all $m_i$. Then $m(\alpha)v = m(\alpha)(w_1+\dots+w_k) = m(\alpha)w_1 + \dots + m(\alpha)w_k = 0$. 
\end{proof}
\iffalse
\begin{proof}
	Let $f = \lcm(m_1, \dots, m_k)$; there exists $n_i$ such that $f(x) = m_i(x)n_i(x)$. Pick $w \in W$; then $f(\alpha)w = 0$, so $f(\alpha) = 0$ on $\sum W_i = V$. To show that $f(x) \mid m_\alpha(x)$, note that $m_\alpha(\alpha) = 0$ on $V$. So 
\end{proof}
\fi
\begin{remark}
	We would see later that $\chi_\alpha(x) = \Pi \chi_{A\vert_{W_i}}(x)$.
\end{remark}
\begin{theorem}
	Let $\alpha \in \L(V)$, $\dim{V} = n < \infty$. Then $\deg{m_\alpha} \le n$.
\end{theorem}
\begin{proof}
	To find $p(x) \in F[x]$ such that $p(\alpha)=0$ and $\deg{p} \le n$. If $\dim{V} = 1$, then $\alpha(v) = kv$, so $p(x) := x-k$; $\alpha$ satisfies $p(x)$. 
	\par Now we proceed by induction; assume the claim is true for all finite-dimensional having $\dim < n$. 
	\par Do induction on $n$; pick $v\ne0 \in V$. Consider $\{v, \alpha(v), \dots, \alpha^n(v)\} \subset V$. Then $\sum c_ia^i(v)=0$, so $p(\alpha)v=0$. Now let $W = \ker p(\alpha) \subset V$. Observe that if $\alpha \circ \beta = \alpha \circ \beta$, then $\ker(\alpha)$ and $\im(\alpha)$ are $\beta-$invariant. Now apply this to $\alpha$ and $p(\alpha)$, we get that $W$ is $\alpha$-invariant. Then the map $\tilde{\alpha}: V/W \to V/W$, $\tilde{\alpha}(v+W)=\alpha(v)+W$ is well-defined. Let $q = m_{\tilde{\alpha}}$ and $r = m_{\alpha \vert_W}$. By hypothesis $\deg q \le \dim{V/W} = \dim{V}-\dim{W}$, $\deg{r} \le \dim{W}$. Now we claim that $q(\alpha)r(\alpha)v=0$ for all $v \in V$. Indeed, because $q(\alpha)r(\alpha)v = r(\alpha)q(\alpha)v$, it sufficies to show that $q(\alpha)v \in W$, i.e. $q(\alpha)v+W = 0+W$. By definition of $q$, $q(\alpha)v+W = q(\tilde{\alpha})(v+W) = 0+W$. Finally, $\deg{q(\alpha)r(\alpha)} \le \dim{V}-\dim{W}+\dim{W} = n$, completing the proof.
\end{proof}
\begin{remark}
	This also follows from Theorem \ref{CH} later.
\end{remark}
\iffalse
We now record a lemma that will be repeatedly useful later.
\begin{lemma} \label{minpoly}
	Let $\alpha \in \L(V)$. Suppose that $\alpha$ satisfies $g_1(x) \dots g_n(x)$ where $g_i \in F[x]$ are monic with $\gcd(g_1, \dots, g_n)=1$. Then
	\begin{enumerate}
		\item $W_i := \ker(g_i(\alpha)) = \im(\Pi_{j \ne i}g_j(\alpha))$
		\item $g_i(\alpha) \vert_{W_i}$ is bijective (on $W_i$) for all $i$
		\item $V = \bigoplus W_i$.
	\end{enumerate}
	Furthermore, if $m_\alpha(x)=g_1(x) \dots g_n(x)$, then $m_{\alpha \vert_{W_i}} = g_i(x)$.
\end{lemma} 
\begin{proof}
	\begin{enumerate}
		\item We have $g_i(\alpha)\Pi_{j \ne i}g_j(\alpha) = 0$, so $\im(\Pi_{j \ne i}g_j(\alpha)) \subset \ker(g_i(\alpha))$; from \ref{bezout}, there exists $f_1, \dots f_n \in F[x]$ such that 
		\[f_1(x)g_1(x) + \dots + f_n(x)g_n(x) = 1 \implies f_1(\alpha)g_1(\alpha) + \dots + f_n(\alpha)g_n(\alpha) = \id.\]
		Let $v \in \ker(g_i(\alpha))$, then $v = $
	\end{enumerate}
\end{proof}
\fi
We accept the basic definition and properties of determinants for now. A detailed discussion is deferred to later. The goal is to define a map
\begin{align*}
	F[x] &\to \L(V) \\
	p(x) &\mapsto p(\alpha)
\end{align*}
to `decompose' $V, \alpha$. 
\begin{definition}
	Let $R$ be a commutative ring with $1_R$. Let $A=(a_{ij}) \in M_n(R)$. Then 
	\[\det(A) = \sum_{\sigma \in S_n} \sgn(\sigma)a_{1\sigma(1)}a_{2\sigma(2)}\dots a_{n\sigma(n)}\]
\end{definition}
(We would not digress on the relevant facts in group theory. )
\subsection{Eigenstuff}
\begin{definition}
	Let $\alpha: V \to V$, $\lambda \in F$. Define $E_\lambda := \{v \in V: \alpha(v) = \lambda v\}$. If $E_\lambda \ne \{0\}$, then $\lambda$ is an eigenvalue of $\alpha$; any vector $v \in E_\lambda-\{0\}$ is an associated eigenvector.
\end{definition}
One can check that $E_\lambda$ is a subspace. If $W$ is a $1-$dimensional invariant subspace, then there exists $\lambda$ such that $W = E_\lambda$. 
\begin{theorem}
	Let $V$ be finite-dimensional. The following are equivalent: 
	\begin{enumerate}
		\item $\lambda$ is an eigenvalue
		\item $\alpha - \lambda I$ is not invertible
		\item $\det(A-\lambda I) = 0$ for all $A = [\alpha]_\B$ where $\B$ is some basis of $V$.
	\end{enumerate}
\end{theorem}
\begin{remark}
	Note that if $\B, \B'$ are two bases of $V$, then $[\alpha]_\B = X^{-1}[\alpha]_{\B'}X$ for $X = [\id]_{\B', \B}$. Then
	\[\det([\alpha]_\B-\lambda I) = \det(X^{-1}[\alpha]_{\B'}X-\lambda I) =  \det(X^{-1}([\alpha]_{\B'}-\lambda I)X) = \det([\alpha]_{\B'}-\lambda I).\]% = X^{-1}([\alpha]_{\B'}-\lambda I)X.\]
\end{remark}
\begin{definition}
	Let $V$ be a finite dimensional vector space and $\alpha \in \L(V)$. The characteristic polynomial $\chi_\alpha(x) := \det(x\id-\alpha)$.
\end{definition}
\begin{remark}
	One can check that characteristic polynomial is a continuous function of the matrix. But the minimal polynomial is not (consider $\begin{pmatrix}
		1 & \epsilon \\
		0 & 1
	\end{pmatrix}$ and $I$). 
\end{remark}
Next, we see that the characteristic polynomial and minimal polynomial have the same roots:
\begin{proposition}
	Let $\alpha \in \L(V)$. The following are equivalent:
	\begin{enumerate}
		\item $\lambda$ is an eigenvalue of $\alpha$
		\item $\lambda$ is a root of $\chi_\alpha$
		\item $\lambda$ is a root of $m_\alpha$.
	\end{enumerate}
\end{proposition}
\begin{proof}
	We know that 1. and 2. are equivalent, so it suffices to show that $m_\alpha(\lambda) = 0$ if and only if $\lambda$ is an eigenvalue. Suppose that $m_\alpha(\lambda) = 0$, then we can write $m_\alpha(x) = (x-\lambda)q(x)$ where $\deg{q}<\deg{m_\alpha}$. Since $q(\alpha) \ne 0$, there exists $v \in V$ such that $q(\alpha)v \ne 0$. Then $(\alpha-\lambda \id)(q(\alpha)v) = m_\alpha(\alpha)v = 0$ shows that $q(\alpha)v$ is an eigenvcetor with eigenvalue $\lambda$. Conversely, suppose that $\lambda$ is an eigenvalue with eigenvector $v$. Observe that for $p \in F[x]$, $p(\alpha)v = p(\lambda)v$. In particular, $m_\alpha(\alpha)v = m_\alpha(\lambda)v = 0$, so $m_\alpha(\lambda)=0$.
\end{proof}
\begin{example}
	Let $A = \begin{pmatrix}
		5 & -6 & -6 \\
		-1 & 4 & 2 \\
		3 & -6 & -4
	\end{pmatrix}$. Then $\chi_A(x) = (x-2)^2(x-1)$. By above, $1, 2$ are roots of $m_A$. It turns out that $m_A(x) = (x-1)(x-2)$.
\end{example}
\begin{definition}
	Let $V$ be a finite dimensional vector space and $\alpha \in \L(V)$. We say that $\alpha$ is diagonalisable if there exists a basis of $V$ consisting of eigenvectors; equivalently, there exists $P$ invertible such that $P^{-1}AP$ is a diagonal matrix. 
\end{definition}
\begin{proposition}
	Let $\alpha \in \L(V)$ and $\lambda_1, \dots, \lambda_k$ be distinct eigenvalues; then $\sum_{i=1}^k E_{\lambda_i}$ is in fact a direct sum $\bigoplus_{i=1}^k E_{\lambda_i}$.
\end{proposition}
\begin{proof}
	Let $v_i \in E_{\lambda_i}$ for $1\le i \le k$ be such that $v_1+\dots+v_k=0$. Observe that for any $p(x) \in F[x]$, 
	\[p(\alpha)(v_1+\dots+v_k) = p(\alpha)v_1+\dots+p(\alpha)v_k = p(\lambda_1)v_1+\dots+p(\lambda_k)v_k = 0.\]
	But we can find polynomials $p_i$ such that $p_i(\lambda_i) = 1$ and $p_i(\lambda_j) = 0$ for all $j \ne i$. Applying these $p_i$ to above then gives $v_i = 0$ for all $i$.
\end{proof}
%(Actually an easier way is probably use the independence of $v_i$. )
\begin{corollary}
	If $V = \sum E_{\lambda_i}$ for distinct $\lambda_i$, then $\B := \bigcup \B_{E_{\lambda_i}}$ (where $\B_{E_{\lambda_i}}$ is basis for $E_{\lambda_i}$) is a basis of $V$ full of eigenvectors, i.e. $\alpha$ is diagonalisable.
\end{corollary}
\begin{corollary}
	If $\alpha$ is diagonalisable, then $V=\bigoplus_{i=1}^k E_{\lambda_i}$ where $E_{\lambda_i}$ are eigenspaces for distinct eigenvalues $\lambda_i$.
\end{corollary}
\begin{proof}
	This follows from observing that $\dim{V} \le \sum \dim E_{\lambda_i}$.
\end{proof}
\begin{lemma} \label{properinv}
	Suppose $m_\alpha(x) = (x-c_1)^{r_1}\dots (x-c_k)^{r_k}$ for $c_i \in F$, $r_k \in \Z_{>0}$. Let $W$ be a proper $\alpha$-invariant subspace. Then there exists $v \notin W$ such that $(\alpha-c_i\id)v \in W$ for some $i$. 
\end{lemma}
\begin{proof}
	Since $W$ is $\alpha$-invariant, we have the usual map $\tilde{\alpha}: V/W \to V/W$, $\tilde{\alpha}(v+W) = \alpha(v) + W$. From Corollary \ref{dividem}, we have $m_{\tilde{\alpha}} \mid m_\alpha$, so there is some $i$ such that $c_i$ is a eigenvalue for $\tilde{\alpha}$. Now let $v+W$ be a corresponding eigenvector. We have
	\[\alpha(v) + W = \tilde{\alpha}(v+W) = c_i(v+W) = c_iv + W \implies \alpha(v)-c_iv=(\alpha-c_i\id)v \in W\]
	and $v \notin W$ because $v+W \ne 0+W$. 
\end{proof}
\begin{theorem}
	Let $V$ be a vector space of dimension $n < \infty$ and $\alpha \in \L(V)$. Then $\alpha$ is diagonalisable if and only if $m_\alpha(x) = (x-c_1)\dots(x-c_k)$ for distinct eigenvalues $c_i$ and $k \le n$. 
\end{theorem}
\begin{proof}
	($\Rightarrow$) There exists $\B = \{v_1, \dots v_n\}$ basis of $V$ such that $v_i$ are eigenvectors. Let $p(x) = (x-c_1)\dots(x-c_k)$. To show that $p(\alpha)=0$, it suffices to check that $p(\alpha)v_i = 0$ for all $i$. But let $c$ be the eigenvalue of $v_i$, we see that $(\alpha-c\id)v_i = 0$. Moreover, all eigenvalues are roots of $m_\alpha$, $p$ must be $m_\alpha$ itself. 
	\par ($\Leftarrow$) Consider $W = \bigoplus E_\lambda \subset V$. Note that $W$ is $\alpha$-invariant. Suppose that $W \ne V$, then apply Lemma \ref{properinv} to get $v \notin W$ such that $w:= (\alpha-c\id)v \in W$. Write $w = w_1+\dots+w_k$ where $\alpha(w_i) = c_iw_i$. Then for any $h(x) \in F[x]$, we have $h(\alpha)w = h(c_1)w_1+\dots h(c_k)w_k$. Write $m_\alpha(x) = (x-c_i)q(x)$. We can write $q(x)-q(c_i) = (x-c_i)h(x)$. We have $q(\alpha)v - q(c_i)v = h(\alpha)(\alpha-c_i\id)v = h(\alpha)w \in W$. Note that $(\alpha-c_i\id)q(\alpha)v = m_\alpha(\alpha)v = 0$, so $q(\alpha)v \in W$, so $q(c_i)v \in W$. But $v \notin W$, so $q(c_i)=0$, which is a contradiction. 
\end{proof}
\begin{remark}
	This also follows from Theorem \ref{primary} later.
\end{remark}
\iffalse
Another proof? Let $m_\alpha(x) = (x-\lambda_1)\dots(x-\lambda_k)$, where $\lambda_i$ are distinct. We want to show that $V = \bigoplus E_{\lambda_i}$. It suffices to show $V = \sum E_{\lambda_i}$, i.e. to exhibit $v = v_1+\dots+v_k$ with $v_i \in E_{\lambda_i}=\ker(\alpha-\lambda_i\id)$. This would hold if there exists $h_1, \dots h_k \in F[x]$ such that $1 = h_1(x)+\dots+h_k(x)$ and $(x-\lambda_i)h_i(x)$ is divisible by $m_\alpha(x)$. Because, the above would show that $\id = h_1(\alpha)+\dots+h_k(\alpha) \implies v = h_1(\alpha)v+\dots+h_k(\alpha)v$ and $(\alpha-\lambda_i)h_i(\alpha)v = 0$. Now we check the claim. Let $f_i(x) = \frac{m_\alpha(x)}{x-\lambda_i}$. Then $\gcd(h_1, \dots h_k) = 1$, so by Bezout, $\sum f_i(x)g_i(x) = 1$. Then taking $h_i(x) = f_i(x)g_i(x)$ finishes the proof. 
\fi
This theorem provides an efficient way to check whether $\alpha$ is diagonalisable. Say $\chi_\alpha(x) = (x-1)^3(x-2)^2$. Then we just check whether $(\alpha-1)(\alpha-2) = 0$.
\begin{example}
	\begin{enumerate}
		\item $A = \begin{pmatrix}
			0 & -1 \\
			1 & 0
		\end{pmatrix}$. Then $\chi_A(x) = x^2+1$, so no real eigenvalues. One can check that $m_\alpha = x^2+1$ (e.g. by Cayley-Hamilton later).
		\item $B = \begin{pmatrix}
			1 & 1 \\
			0 & 1
		\end{pmatrix}$ Then $\chi_B(x) = (x-1)^2$ and $m_\beta=(x-1)^2$. Indeed, $B$ is also not diagonalisable.
	\end{enumerate}
\end{example}
\begin{definition}
	We say that $\alpha \in \L(V)$ is traingularisable if there exists some basis $\B$ of $V$ such that $[\alpha]_\B$ is traingular. 
\end{definition}
\begin{theorem}
	$\alpha$ is traingularisable if and only if $m_\alpha(x) = (x-c_1)^{r_1}\dots (x-c_k)^{r_k}$ for $c_i \in F$, $r_k \in \Z_{>0}$.
\end{theorem}
\begin{proof}
	($\Rightarrow$) Let $[\alpha]_\B = (a_{ij})$, then $\chi_\alpha(x) = (x-a_{11})\dots(x-a_{nn}) = (x-c_1)^{t_1}\dots (x-c_k)^{t_k}$. By Theorem \ref{CH}, $m_\alpha \mid \chi_\alpha$, so $m_\alpha(x) = (x-c_1)^{r_1}\dots (x-c_k)^{r_k}$ where $r_i \le t_i$.
\par ($\Leftarrow$) $\alpha$ being traingularisable with repsect to $\B = \{v_1, \dots v_n\}$ means that $\alpha(v_i) \in \spn\{v_1, \dots v_i\}$. Apply Lemma \ref{properinv} to $W = \{0\}$ to obtain $v_1$. Then apply lemma again to $W = \spn\{v_1\}$ (which is $\alpha$-invariant) to obtain $v_2$. We see that doing this repeatedly yields the desired $v_i$. 
\end{proof}
\begin{remark}
	Actually, this is equivalent to Theorem \ref{CH}. However, ($\Leftarrow$) does not use Theorem \ref{CH} and we actually use this direction to prove it below. 
\end{remark}
\begin{corollary}
	If $F$ is algebraically closed, then every linear operator on a finite-dimensional $F$-vector space $V$ is triangularisable. 
\end{corollary}
We take without proof the following theorem:
\begin{theorem}
	Given any field $F$, there exists a field $\bar{F}$ such that $F \subset \bar{F}$ and $\bar{F}$ is algebraically closed. 
\end{theorem}

\begin{theorem}[Cayley-Hamilton] \label{CH}
	$\chi_\alpha(\alpha) = 0$.
\end{theorem}
\begin{proof}
	Passing to $\bar{F}$, split $m_\alpha(x) = \Pi (x-\lambda_i)$ where $\lambda_i \in \bar{F}$. Then $\alpha$ is traingularisable in $M_n(\bar{F})$. Now we observe that $\chi_A(A) = 0$ for upper triangular $A$ (`killing' row by row from the bottom). Since $\chi_A \in F[x]$, the theorem follows. 
\end{proof}
\begin{corollary}
	$m_\alpha \mid \chi_\alpha$. 
\end{corollary}
%Let $\chi_A(x) = x^n+a_{n-1}x^{n-1}+\dots+a_1x+a_0$. Then $\text{trace}=-a_{n-1}$ and $a_i$ are polynomial of the $\text{trace}(A^k)$.
\subsection{Canonical forms}
\begin{theorem}[primary decomposition] \label{primary}
	$\alpha \in \L(V)$; let $m_\alpha = p_1^{r_1}\dots p_k^{r_k}$ where $p_i$ are distinct irreducible monic polynomials in $F[x]$. Set $W_i = \ker{p_i^{r_i}(\alpha)}$. Then
	\begin{enumerate}
		\item $V = \bigoplus W_i$
		\item $W_i$ is $\alpha$-invariant
		\item $m_{\alpha\vert_{W_i}} = p_i^{r_i}$ for all $i$.
	\end{enumerate}
	Moreover, our $W_i$ is the unique collection of subspaces satisfying the above three conditions.
\end{theorem} 
\begin{proof}
	\begin{enumerate}
		\item Set $f_i(x) = \frac{m_\alpha(x)}{p_i(x)^{r_i}}$, then $\gcd(f_1, \dots, f_k)=1$, so by Lemma \ref{bezout} we can write
		\[1=\sum_{i=1}^k g_i(x)f_i(x)\]
		for some $g_i \in F[x]$. Evaluating at $\alpha$ gives
		\[\id = \sum_{i=1}^k h_i(\alpha) \implies v = \sum_{i=1}^k h_i(\alpha)v\]
		where we write $h_i(\alpha) := f_i(\alpha)g_i(\alpha)$. Now observe that $m_\alpha \mid p_i(x)^{r_i}h_i(x)$, showing that $h_i(\alpha)v \in W_i$. Thus $V = \sum W_i$. Now suppose for contradiction that it is not a direct sum. Then there exists a shortest list of length $l \ge 2$ such that
		\[w_1 + \dots + w_l = 0\]
		where $w_j$ are all non-zero and from distinct $W_i$. Now say $w_1 \in W_i$. Then 
		\[p_i^{r_i}(\alpha)(w_1 + \dots + w_l) = p_i^{r_i}(\alpha)w_2 + \dots p_i^{r_i}(\alpha)w_l = 0\]
		where crucially, $p_i^{r_i}(\alpha)w_j \ne 0$ for $2 \le j \le l$: this is because otherwise, say there exists $w_n \in W_j$, $2 \le n \le l$ such that $p_i^{r_i}(\alpha)w_n = 0$, then it contradicts Lemma \ref{bezout} applied to $p_i^{r_i}$ and $p_j^{r_j}$. This contradicts the minimality of $l$.
		\item This is obvious.
		\item It is clear that $m_{\alpha\vert_{W_i}} \mid p_i^{r_i}$. Moreover, if $m_{\alpha\vert_{W_i}} = p_i^{r'_i}$ for some $r'_i < r_i$, then it contradicts the minimality of $m_\alpha$. 
	\end{enumerate}	
	For uniqueness, note that from 3. we get that $W_i \subset \ker{p_i^{r_i}}(\alpha)$.
\end{proof}

\begin{corollary}[Jordan-Chevalley decomposition]
	Let $F$ be algebraically closed; $\alpha \in \L(V)$ where $V$ is finite-dimensional; then there exists a decomposition $\alpha = D+N$ where $D$ is diagonal, $N$ is nilpotent and $DN = ND$. 
\end{corollary}
\begin{proof}
	$m_\alpha(x) = (x-\lambda_1)^{r_1} \dots (x-\lambda_k)^{r_k}$. We have from above, 
	\[V = \bigoplus_{i=1}^k \ker(\alpha-\lambda_i\id)^{r_i}\] 
	So we collect the corresponding bases $\B = \bigcup \B_i$. Then
	\[[\alpha_\B]=\begin{pmatrix}
		A_1 & 0 & \cdots & 0 \\
		0 & A_2 & \cdots & 0 \\
		%0 & 0 & \cdots & 0 \\
		\vdots & \vdots & \ddots & \vdots \\
		0 & 0 & \cdots & A_k
	\end{pmatrix} = \begin{pmatrix}
		\lambda_1 I & \cdots & 0 \\
		\vdots & \ddots & \vdots \\
		0 & \cdots & \lambda_k I
	\end{pmatrix} + N\]
	where $(A_i-\lambda_i I)^{r_i}=0$, so $N_i := A_i-\lambda_i I$ is nilpotent, implying that $A_i = \lambda_i I + N_i$. We note finally that $ND = DN$.
\end{proof}
Todo: tidy up below. 
\par Random: consider the projection maps for $V = \bigoplus W_i$. Then the decomposition is $\alpha$-invariant if and only if $\pi_i\alpha = \alpha \pi_i$.
\par Now we focus on the simplest $\alpha$-invariant subspaces i.e. the cyclic subspaces $\langle v \rangle_\alpha := \{p(\alpha): p \in F[x]\} = \spn\{\alpha^iv: i \in \Z_{>0}\}$. We have the following algorithm: form $\{v, Av, \dots, A^iv\}$ until it first becomes linearly dependent. Then $\sum_{i=0}^kc_iA^iv = 0$, and we get the minimal polynomial of $\langle v \rangle_\alpha$.
\begin{proposition}
	\begin{enumerate}
		\item $\langle v \rangle_\alpha$ is finite dimensional if and only if there exists $p(x) \in F[x]-\{0\}$ such that $p(\alpha)v=0$. We define $m_{\alpha, v}(x)$ to be the unique such monic polynomial of least degree. 
		\item Let $\dim{\langle v \rangle_\alpha} = n$. Then $\deg{m_{\alpha, v}(x)} = n$. 
		\item $\chi_{\alpha \vert_{\langle v \rangle_\alpha}}(x) = m_{\alpha\vert_{\langle v \rangle_\alpha}}(x) = m_{\alpha, v}(x)$.
		\item $\B = \{v, \alpha(v), \dots, \alpha^{n-1}(v)\}$ is a basis of $\langle v \rangle_\alpha$.
		\item $[\alpha \vert_{\langle v \rangle_\alpha}]_\B = \begin{pmatrix}
			0 & 0 & \cdots & 0 & -a_0 \\
			1 & 0 & \cdots & 0 & -a_1 \\
			0 & 1 & \cdots & 0 & -a_2 \\
			\vdots & \vdots & \ddots & \vdots & \vdots \\
			0 & 0 & \cdots & 1 & -a_{n-1} 
		\end{pmatrix}$, where $m_{\alpha, v}(x) = x^n+\sum_{i=0}^{n-1}a_ix^i$. This is the companion matrix $C(m_{\alpha, v}(x))$. 
	\end{enumerate}
\end{proposition}
\begin{proof}
	We only prove 3.:
\end{proof}
We introduce the notation $\{v\}_\alpha^n := \{v, \alpha v, \dots \alpha^{n-1}v\}$, so that $\langle v \rangle_\alpha := \spn\{v\}_\alpha^n$.
\begin{lemma}
	Let $\alpha \in \L(V)$ be such that $m_\alpha = f(x)^k$ for $f(x)$ monic irreducible of degree $n$. Let $v \in \ker f(\alpha)^k - \ker f(\alpha)^{k-1}$ (which is nonempty by minimality of $m_\alpha$). \begin{enumerate}
		\item $m_{\alpha, v}(x) = f(x)^k$
		\item $\bigcup_{i=0}^{k-1}\{f(\alpha)^iv\}_\alpha^n$ is a basis of $\langle v \rangle_\alpha$.
		\item $\bigcup_{i=k-j}^{k-1}\{f(\alpha)^iv\}_\alpha^n$ is a basis of $\langle v \rangle_\alpha \cap \ker{f(\alpha)^j}$, for all $j = 1, \dots, k$. 
	\end{enumerate}
\end{lemma}
\begin{remark}
	The setting is that $V$ having a basis $\B = \{v_{11}, \dots v_{nk}\}$ and we want to decompose into the flag
	\[\ker{f(\alpha)} \subset \ker{f(\alpha)}^2 \subset \dots \subset \ker{f(\alpha)}^k = V.\]
\end{remark}
\begin{proof}
	\begin{enumerate}
		\item Since $m_{\alpha, v}(x) \mid m_\alpha(x)$ for any $v \in V$. So $m_{\alpha, v}(x) = f(x)^j$ for some $j \le k$. But note that $j \ge k$, otherwise 
		\item Note $P_{nk-1} := \{p(x) \in F[x]: \deg{p} \le nk-1\}$ has two bases
		\[\{1, x, \dots, x^{nk-1}\}\]
		and (note that $\deg{f}=n$)
		\[\{x^jf(x)^i: 0 \le j \le n-1, 0 \le i \le k-1\}.\]
		We can write $x^l$ in terms of $x^jf(x)^i$
		\item tutorial?
	\end{enumerate}
\end{proof}
\begin{lemma}
	Let $v_1, \dots, v_r \in \ker{f(\alpha)^l}$ such that
	\[v_i \notin \ker{f(\alpha)^{l-1}} = \sum_{j=1}^{i-1} \langle v_j \rangle.\]
	Then 
\end{lemma}

\end{document}